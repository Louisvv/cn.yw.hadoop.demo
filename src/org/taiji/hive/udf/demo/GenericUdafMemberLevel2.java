package org.taiji.hive.udf.demo;

import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.commons.logging.Log;  
import org.apache.commons.logging.LogFactory;  
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;  
import org.apache.hadoop.hive.ql.metadata.HiveException;  
import org.apache.hadoop.hive.ql.parse.SemanticException;  
import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;  
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFParameterInfo;  
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver2;  
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;  
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;  
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;  
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;  
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;  
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;  
import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;  
import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;    
import org.apache.hadoop.util.StringUtils;  
  
public class GenericUdafMemberLevel2 extends AbstractGenericUDAFResolver {  
    private static final Log LOG = LogFactory  
            .getLog(GenericUdafMemberLevel2.class.getName());  
      
    @Override  
      public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)  
        throws SemanticException {  
          
        return new GenericUdafMeberLevelEvaluator();  
      }  
      
    public static class GenericUdafMeberLevelEvaluator extends GenericUDAFEvaluator {  
        private PrimitiveObjectInspector inputOI;  
        private PrimitiveObjectInspector inputOI2;  
        private PrimitiveObjectInspector outputOI;  
        private DoubleWritable result;  
  
        @Override  
        public ObjectInspector init(Mode m, ObjectInspector[] parameters)  
                throws HiveException {  
            super.init(m, parameters);  
              
            //init input  
            if (m == Mode.PARTIAL1 || m == Mode.COMPLETE){ //必须得有  
                LOG.info(" Mode:"+m.toString()+" result has init");  
                inputOI = (PrimitiveObjectInspector) parameters[0];  
                inputOI2 = (PrimitiveObjectInspector) parameters[1];  
//              result = new DoubleWritable(0);  
            }  
            //init output  
            if (m == Mode.PARTIAL2 || m == Mode.FINAL) {  
                outputOI = (PrimitiveObjectInspector) parameters[0];  
                result = new DoubleWritable(0);  
                return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;  
            }else{  
                result = new DoubleWritable(0);  
                return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;  
            }   
              
        }  
  
        /** class for storing count value. */  
        static class SumAgg implements AggregationBuffer {  
            boolean empty;  
            double value;  
        }  
  
        @Override  
        //创建新的聚合计算的需要的内存，用来存储mapper,combiner,reducer运算过程中的相加总和。  
        //使用buffer对象前，先进行内存的清空――reset  
        public AggregationBuffer getNewAggregationBuffer() throws HiveException {  
            SumAgg buffer = new SumAgg();  
            reset(buffer);  
            return buffer;  
        }  
  
        @Override  
        //重置为0  
        //mapreduce支持mapper和reducer的重用，所以为了兼容，也需要做内存的重用。  
        public void reset(AggregationBuffer agg) throws HiveException {  
            ((SumAgg) agg).value = 0.0;  
            ((SumAgg) agg).empty = true;  
        }  
  
        private boolean warned = false;  
        //迭代  
        //只要把保存当前和的对象agg，再加上输入的参数，就可以了。  
        @Override  
        public void iterate(AggregationBuffer agg, Object[] parameters)  
                throws HiveException {  
            // parameters == null means the input table/split is empty  
            if (parameters == null) {  
                return;  
            }  
            try {  
                double flag = PrimitiveObjectInspectorUtils.getDouble(parameters[1], inputOI2);  
                if(flag > 1.0)   //参数条件  
                    merge(agg, parameters[0]);   //这里将迭代数据放入combiner进行合并  
              } catch (NumberFormatException e) {  
                if (!warned) {  
                  warned = true;  
                  LOG.warn(getClass().getSimpleName() + " "  
                      + StringUtils.stringifyException(e));  
                }  
              }  
  
        }  
  
        @Override  
        //这里的操作就是具体的聚合操作。  
        public void merge(AggregationBuffer agg, Object partial) {  
            if (partial != null) {  
                // 通过ObejctInspector取每一个字段的数据  
                if (inputOI != null) {  
                    double p = PrimitiveObjectInspectorUtils.getDouble(partial,  
                            inputOI);  
                    LOG.info("add up 1:" + p);  
                    ((SumAgg) agg).value += p;  
                } else {  
                    double p = PrimitiveObjectInspectorUtils.getDouble(partial,  
                            outputOI);  
                    LOG.info("add up 2:" + p);  
                    ((SumAgg) agg).value += p;  
                }  
            }  
        }  
  
  
        @Override  
        public Object terminatePartial(AggregationBuffer agg) {  
                return terminate(agg);  
        }  
          
        @Override  
        public Object terminate(AggregationBuffer agg){  
            SumAgg myagg = (SumAgg) agg;  
            result.set(myagg.value);  
            return result;  
        }  
    }  
}  